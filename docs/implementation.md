# Implementation Guide â€” Data Flow & Architecture

This document explains how data flows through the RAG pipeline, what each module does, and how they connect together.

---

## Table of Contents

1. [Architecture Overview](#architecture-overview)
2. [Ingestion Pipeline](#ingestion-pipeline)
3. [Query Pipeline](#query-pipeline)
4. [Module Reference](#module-reference)
5. [Data Storage](#data-storage)
6. [Configuration System](#configuration-system)
7. [Logging System](#logging-system)
8. [Frontend Architecture](#frontend-architecture)

---

## Architecture Overview

The system uses a **modular pipeline architecture** where each stage of the RAG workflow is handled by a dedicated module in `src/rag/`. The `query_engine.py` acts as a thin coordinator that wires all modules together, and `app.py` provides the Streamlit UI.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        STREAMLIT UI (app.py)                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚   â”‚ ğŸ’¬ Chat  â”‚     â”‚ ğŸ“ Documents â”‚     â”‚ ğŸ“Š Visualize â”‚       â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                  â”‚                    â”‚
         â–¼                  â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  QUERY ENGINE (query_engine.py)                  â”‚
â”‚                     Thin Coordinator Layer                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                   â”‚                    â”‚
   QUERY PIPELINE      INGEST PIPELINE     VISUALIZATIONS
        â”‚                   â”‚                    â”‚
        â–¼                   â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Retriever   â”‚  â”‚ Loaderâ†’Chunkerâ†’  â”‚  â”‚  Visualizer  â”‚
â”‚  + Generator â”‚  â”‚ Embedderâ†’Store   â”‚  â”‚  (Plotly)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Ingestion Pipeline

**Purpose:** Transform a raw PDF file into searchable vector embeddings stored in ChromaDB.

### Step-by-Step Data Flow

```
PDF File
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: LOADING (src/rag/loader.py â†’ SimplePDFLoader)        â”‚
â”‚                                                              â”‚
â”‚ Input:  file_path (string)                                   â”‚
â”‚ Action: Opens PDF, extracts text page-by-page using pypdf    â”‚
â”‚ Output: {                                                    â”‚
â”‚           'filename': 'document.pdf',                        â”‚
â”‚           'pages': [                                         â”‚
â”‚             {'page_number': 1, 'text': '...', 'length': 847},â”‚
â”‚             {'page_number': 2, 'text': '...', 'length': 1203}â”‚
â”‚           ],                                                 â”‚
â”‚           'metadata': {                                      â”‚
â”‚             'total_pages': 35,                               â”‚
â”‚             'total_characters': 28450,                       â”‚
â”‚             'filename': 'document.pdf'                       â”‚
â”‚           }                                                  â”‚
â”‚         }                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: CHUNKING (src/rag/chunker.py â†’ TextChunker)          â”‚
â”‚                                                              â”‚
â”‚ Input:  PDF data dict from Step 1                            â”‚
â”‚ Action: Splits text into overlapping chunks                  â”‚
â”‚         - chunk_size: 800 chars (configurable)               â”‚
â”‚         - chunk_overlap: 150 chars (configurable)            â”‚
â”‚         - Preserves page and source metadata per chunk       â”‚
â”‚ Output: List of chunk dicts:                                 â”‚
â”‚         [                                                    â”‚
â”‚           {                                                  â”‚
â”‚             'text': 'chunk content here...',                 â”‚
â”‚             'chunk_index': 0,                                â”‚
â”‚             'length': 798,                                   â”‚
â”‚             'source': {                                      â”‚
â”‚               'filename': 'document.pdf',                    â”‚
â”‚               'page_number': 1                               â”‚
â”‚             }                                                â”‚
â”‚           },                                                 â”‚
â”‚           ...                                                â”‚
â”‚         ]                                                    â”‚
â”‚ Side Effect: Saved to data/chunks/<filename>_chunks.json     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: EMBEDDING (src/rag/embedder.py â†’ EmbeddingGenerator) â”‚
â”‚                                                              â”‚
â”‚ Input:  List of chunk dicts from Step 2                      â”‚
â”‚ Action: Converts each chunk's text into a 384-dim vector     â”‚
â”‚         using sentence-transformers (all-MiniLM-L6-v2)       â”‚
â”‚ Output: {                                                    â”‚
â”‚           'embeddings': numpy.ndarray (shape: [N, 384]),     â”‚
â”‚           'chunks': [...original chunk dicts...],            â”‚
â”‚           'metadata': {                                      â”‚
â”‚             'model_name': 'all-MiniLM-L6-v2',               â”‚
â”‚             'dimension': 384,                                â”‚
â”‚             'total_chunks': N                                â”‚
â”‚           }                                                  â”‚
â”‚         }                                                    â”‚
â”‚ Side Effect: Saved to data/embeddings/<filename>_embeddings  â”‚
â”‚              (.npy for vectors, _metadata.json for chunks)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: STORAGE (src/rag/vector_store.py â†’ VectorStore)      â”‚
â”‚                                                              â”‚
â”‚ Input:  Embedded data dict from Step 3                       â”‚
â”‚ Action: Stores vectors + metadata in ChromaDB collection     â”‚
â”‚         - Collection name: 'knowledge_base'                  â”‚
â”‚         - Persists to: ./vector_db/                          â”‚
â”‚         - Batches inserts for performance                    â”‚
â”‚ Output: ChromaDB collection with searchable vectors          â”‚
â”‚                                                              â”‚
â”‚ Stored per document:                                         â”‚
â”‚   - id: unique chunk identifier                              â”‚
â”‚   - embedding: 384-dim float vector                          â”‚
â”‚   - document: original chunk text                            â”‚
â”‚   - metadata: {source_file, page_number, chunk_index}        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Query Pipeline

**Purpose:** Take a user's natural language question, find the most relevant chunks, and generate a structured answer with citations.

### Step-by-Step Data Flow

```
User Question: "What are the key investment strategies?"
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: RETRIEVAL (src/rag/retriever.py â†’ Retriever)         â”‚
â”‚                                                              â”‚
â”‚ Substep A â€” Embed Query:                                     â”‚
â”‚   Input:  "What are the key investment strategies?"          â”‚
â”‚   Action: EmbeddingGenerator converts query to 384-dim vectorâ”‚
â”‚   Output: [0.023, -0.145, 0.089, ...]  (384 floats)         â”‚
â”‚                                                              â”‚
â”‚ Substep B â€” Search ChromaDB:                                 â”‚
â”‚   Input:  Query vector + top_k=5                             â”‚
â”‚   Action: Cosine similarity search in ChromaDB               â”‚
â”‚   Output: Top 5 most similar chunks with metadata:           â”‚
â”‚     [                                                        â”‚
â”‚       {                                                      â”‚
â”‚         'text': 'Portfolio diversification involves...',     â”‚
â”‚         'source_file': 'investment-guide.pdf',               â”‚
â”‚         'page_number': 12,                                   â”‚
â”‚         'score': 0.87,                                       â”‚
â”‚         'chunk_index': 45                                    â”‚
â”‚       },                                                     â”‚
â”‚       ...4 more results                                      â”‚
â”‚     ]                                                        â”‚
â”‚                                                              â”‚
â”‚ Optional: filter_source parameter limits search to one file  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: GENERATION (src/services/generator.py â†’ Gemini)      â”‚
â”‚                                                              â”‚
â”‚ Input:  User question + 5 context chunks with metadata       â”‚
â”‚                                                              â”‚
â”‚ Action: Constructs a prompt that includes:                   â”‚
â”‚   1. All 5 chunks labeled as [Document 1-5 | Source, Page]   â”‚
â”‚   2. The user's question                                     â”‚
â”‚   3. Formatting instructions for topic/subtopic structure    â”‚
â”‚   4. Rules requiring inline citations [1], [2], etc.         â”‚
â”‚                                                              â”‚
â”‚ Sends to Gemini API â†’ receives structured response           â”‚
â”‚                                                              â”‚
â”‚ Output (example):                                            â”‚
â”‚   ## Key Investment Strategies                               â”‚
â”‚                                                              â”‚
â”‚   An overview paragraph... [1]                               â”‚
â”‚                                                              â”‚
â”‚   ### Diversification                                        â”‚
â”‚   - Spread investments across asset classes [1]              â”‚
â”‚   - Reduce risk through allocation [2]                       â”‚
â”‚                                                              â”‚
â”‚   ### Value Investing                                        â”‚
â”‚   - Focus on undervalued securities [3]                      â”‚
â”‚                                                              â”‚
â”‚   ---                                                        â”‚
â”‚   **References:**                                            â”‚
â”‚   1. *investment-guide.pdf* â€” Page 12                        â”‚
â”‚   2. *investment-guide.pdf* â€” Page 45                        â”‚
â”‚   3. *investment-guide.pdf* â€” Page 8                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Module Reference

| Module | Class | Key Methods | Purpose |
|--------|-------|-------------|---------|
| `src/rag/loader.py` | `SimplePDFLoader` | `load_pdf(filename)` | Extract text from PDFs page by page |
| `src/rag/chunker.py` | `TextChunker` | `chunk_pdf_data(data)`, `save_chunks(...)` | Split text into overlapping chunks |
| `src/rag/embedder.py` | `EmbeddingGenerator` | `embed_chunks(chunks)`, `embed_texts(texts)`, `save_embeddings(...)` | Generate vector embeddings |
| `src/rag/vector_store.py` | `VectorStore` | `connect()`, `store_embedded_data(data)`, `search(vector, n)`, `get_stats()` | ChromaDB CRUD operations |
| `src/rag/retriever.py` | `Retriever` | `retrieve(query, top_k, filter)` | End-to-end: embed query â†’ search â†’ return results with metadata |
| `src/services/generator.py` | `GeminiGenerator` | `generate(query, context_chunks)` | Send context + question to Gemini, get structured answer |
| `src/core/config.py` | `Settings` | Properties for all paths | Centralized pydantic-settings config from `.env` |
| `src/core/logger.py` | â€” | `get_logger(name)` | Rotating file handler to `data/logs/` |

---

## Data Storage

All generated data lives under the `data/` directory (gitignored):

```
data/
â”œâ”€â”€ chunks/
â”‚   â””â”€â”€ document_chunks.json       # Text chunks with metadata
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ document_embeddings.npy    # Numpy arrays of vectors
â”‚   â””â”€â”€ document_metadata.json     # Chunk text + source info
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ rag_app.log                # Rotating application log (5MB max)
â””â”€â”€ visualizations/
    â””â”€â”€ *.html                     # Plotly interactive charts
```

The vector database is stored separately:

```
vector_db/
â””â”€â”€ chroma.sqlite3                 # ChromaDB persistent storage
```

---

## Configuration System

All settings flow from a single source of truth: `src/core/config.py`

```python
from src.core.config import settings

# Access any setting
settings.GOOGLE_API_KEY     # from .env
settings.CHUNK_SIZE          # 800 (default, or from .env)
settings.chunks_dir          # Path('./data/chunks')
settings.embeddings_dir      # Path('./data/embeddings')
```

Settings are loaded in this priority order:
1. Environment variables (highest priority)
2. `.env` file values
3. Default values in `config.py`

---

## Logging System

The logging system (`src/core/logger.py`) provides:

- **Console output** â€” colored, formatted logs during development
- **File logging** â€” rotating log files in `data/logs/rag_app.log`
  - Max file size: 5MB
  - Keeps 3 backup files
- **Per-module loggers** â€” each module gets its own named logger

```python
from src.core.logger import get_logger
logger = get_logger(__name__)

logger.info("Processing document...")
logger.error("Failed to generate embeddings")
```

---

## Frontend Architecture

The Streamlit app (`app.py`) uses a **tabbed layout** with three sections:

### ğŸ’¬ Chat Tab
- Displays conversation history
- Sidebar controls: retrieval count slider, document filter dropdown
- Shows live metrics (chunks in DB, message count)
- Renders Gemini's structured answers with markdown
- Displays citation cards below each answer

### ğŸ“ Documents Tab
- Multi-file PDF uploader with drag-and-drop
- Progress bar during ingestion
- Knowledge base stats card
- List of ingested source files
- Clear all data button

### ğŸ“Š Visualize Tab
- Four visualization types:
  - **2D PCA Plot** â€” scatter plot of chunk embeddings
  - **3D PCA Plot** â€” interactive 3D rotation
  - **Similarity Heatmap** â€” chunk-to-chunk similarity matrix
  - **Statistics Dashboard** â€” chunk length distribution + metrics
- All charts are interactive Plotly (zoom, pan, hover)

---

## Adding New Data Formats

The system is designed for easy extension. To add a new file format (e.g., `.docx`):

1. Add a new loader method in `src/rag/loader.py`
2. Update `query_engine.py`'s `add_document()` to handle the new extension
3. Update the `file_uploader` in `app.py` to accept the new type

The chunking, embedding, and storage stages remain unchanged since they work on plain text.
