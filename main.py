"""
Main Entry Point - Complete RAG Pipeline
==========================================
Stages: PDF ‚Üí Chunks ‚Üí Embeddings ‚Üí ChromaDB
Run: python main.py
"""

from pathlib import Path
from datetime import datetime
import logging

from src.rag.pipeline import PDFPipeline

# ============================================================================
# CONFIGURATION
# ============================================================================

DATA_FOLDER = r"C:\Users\pritam\Desktop\RAG-Project\data"

# Embedding settings
EMBEDDING_CONFIG = {
    'model': 'fast',
    'batch_size': 32,
    'generate': True
}

# Chunking settings
CHUNK_CONFIG = {
    'size': 512,
    'overlap': 50
}

# Vector Store settings  ‚Üê NEW
VECTOR_STORE_CONFIG = {
    'store': True,                          # Set to False to skip ChromaDB
    'collection_name': 'knowledge_base',    # ChromaDB collection name
}

# ============================================================================
# LOGGING SETUP
# ============================================================================

def setup_logging(data_folder):
    """Setup logging configuration"""
    logs_folder = Path(data_folder) / "logs"
    logs_folder.mkdir(parents=True, exist_ok=True)

    log_file = logs_folder / f'pipeline_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    return log_file

# ============================================================================
# MAIN FUNCTION
# ============================================================================

def main():
    """Main function - Run the complete RAG pipeline"""

    # Setup logging
    log_file = setup_logging(DATA_FOLDER)
    logger = logging.getLogger(__name__)

    # ========================================================================
    # HEADER
    # ========================================================================
    print("\n" + "="*70)
    print("üìö RAG PDF PROCESSING PIPELINE")
    print("="*70)
    print(f"üìÅ Data folder:   {DATA_FOLDER}")
    print(f"üìù Log file:      {log_file}")
    print(f"‚úÇÔ∏è  Chunking:      size={CHUNK_CONFIG['size']}, overlap={CHUNK_CONFIG['overlap']}")
    print(f"üß† Embeddings:    {'Enabled (' + EMBEDDING_CONFIG['model'] + ')' if EMBEDDING_CONFIG['generate'] else 'Disabled'}")
    print(f"üóÑÔ∏è  Vector Store:  {'Enabled (' + VECTOR_STORE_CONFIG['collection_name'] + ')' if VECTOR_STORE_CONFIG['store'] else 'Disabled'}")
    print("="*70 + "\n")

    # ========================================================================
    # CHECK FOR PDFs
    # ========================================================================
    data_path = Path(DATA_FOLDER)
    pdf_files = list(data_path.glob("*.pdf"))

    if not pdf_files:
        print("‚ö†Ô∏è  No PDF files found!")
        print(f"   Please add PDFs to: {DATA_FOLDER}\n")
        logger.warning("No PDF files found in data folder")
        return

    # Display found PDFs
    print(f"üìÑ Found {len(pdf_files)} PDF(s):")
    total_size_mb = 0
    for pdf in pdf_files:
        size_mb = pdf.stat().st_size / 1024 / 1024
        total_size_mb += size_mb
        print(f"   ‚Ä¢ {pdf.name} ({size_mb:.2f} MB)")
    print(f"   üìä Total size: {total_size_mb:.2f} MB\n")

    # Confirm
    try:
        input("Press ENTER to start processing (or Ctrl+C to cancel)... ")
    except KeyboardInterrupt:
        print("\n\n‚ùå Cancelled by user\n")
        logger.info("Processing cancelled by user")
        return

    print()
    logger.info("Starting pipeline processing")

    # ========================================================================
    # RUN PIPELINE
    # ========================================================================
    try:
        pipeline = PDFPipeline(
            data_folder=DATA_FOLDER,
            chunk_size=CHUNK_CONFIG['size'],
            chunk_overlap=CHUNK_CONFIG['overlap'],
            embedding_model=EMBEDDING_CONFIG['model'],
            generate_embeddings=EMBEDDING_CONFIG['generate'],
            store_vectors=VECTOR_STORE_CONFIG['store'],              # ‚Üê NEW
            collection_name=VECTOR_STORE_CONFIG['collection_name'],  # ‚Üê NEW
        )

        results = pipeline.run()

        if not results:
            print("\n‚ùå Pipeline failed! Check logs for details.\n")
            logger.error("Pipeline returned no results")
            return

        # ====================================================================
        # DISPLAY RESULTS
        # ====================================================================

        print("\n" + "="*70)
        print("üìä PROCESSING RESULTS")
        print("="*70)

        # ‚îÄ‚îÄ Chunks per file ‚îÄ‚îÄ
        print("\nüì¶ CHUNKS PER FILE:")
        print("‚îÄ"*70)
        for idx, (filename, chunks) in enumerate(results['chunks'].items(), 1):
            avg_size = sum(c['length'] for c in chunks) / len(chunks) if chunks else 0
            print(f"\n{idx}. {filename}")
            print(f"   ‚Ä¢ Total chunks:       {len(chunks)}")
            print(f"   ‚Ä¢ Average chunk size:  {avg_size:.0f} characters")
            print(f"   ‚Ä¢ Min size:            {min(c['length'] for c in chunks) if chunks else 0} chars")
            print(f"   ‚Ä¢ Max size:            {max(c['length'] for c in chunks) if chunks else 0} chars")

        # ‚îÄ‚îÄ Embeddings ‚îÄ‚îÄ
        if results.get('embeddings'):
            print(f"\n{'‚îÄ'*70}")
            print("üß† EMBEDDINGS:")
            print("‚îÄ"*70)
            for filename, data in results['embeddings'].items():
                shape = data['embeddings'].shape
                print(f"\n   ‚Ä¢ {filename}")
                print(f"     Shape:     {shape}")
                print(f"     Dimension: {shape[1]}")
                print(f"     Model:     {data['metadata']['model_name']}")

        # ‚îÄ‚îÄ Vector Store ‚îÄ‚îÄ                                        ‚Üê NEW
        if results.get('vector_store') and not results['vector_store'].get('error'):
            vs = results['vector_store']
            print(f"\n{'‚îÄ'*70}")
            print("üóÑÔ∏è  VECTOR STORE (ChromaDB):")
            print("‚îÄ"*70)
            print(f"   ‚Ä¢ Collection:      {vs.get('collection_name', 'N/A')}")
            print(f"   ‚Ä¢ Chunks stored:   {vs.get('total_stored', 0)}")
            print(f"   ‚Ä¢ Total in DB:     {vs.get('total_in_collection', 0)}")
            print(f"   ‚Ä¢ Storage time:    {vs.get('storage_time', 0):.2f}s")
            print(f"   ‚Ä¢ Location:        {vs.get('persist_directory', 'N/A')}")
        elif results.get('vector_store', {}).get('error'):
            print(f"\n{'‚îÄ'*70}")
            print("üóÑÔ∏è  VECTOR STORE: ‚ùå Failed")
            print(f"   Error: {results['vector_store']['error']}")

        # ‚îÄ‚îÄ Overall Statistics ‚îÄ‚îÄ
        stats = results['stats']
        print(f"\n{'='*70}")
        print("üìà OVERALL STATISTICS")
        print("="*70)
        print(f"   üìö PDFs processed:        {stats['total_pdfs']}")
        print(f"   üìÑ Total pages:           {stats['total_pages']}")
        print(f"   ‚úÇÔ∏è  Total chunks:          {stats['total_chunks']}")
        print(f"   üî§ Total characters:      {stats['total_characters']:,}")
        print(f"   üìä Avg chunks per PDF:    {stats['total_chunks']/max(stats['total_pdfs'],1):.1f}")
        print(f"   üìä Avg chars per chunk:   {stats['total_characters']/max(stats['total_chunks'],1):.0f}")
        print(f"   üß† Embeddings:            {'‚úÖ Generated' if stats.get('embeddings_generated') else '‚è≠Ô∏è  Skipped'}")
        print(f"   üóÑÔ∏è  Vector Store:          {'‚úÖ Stored (' + str(stats.get('vector_count', 0)) + ' docs)' if stats.get('vectors_stored') else '‚è≠Ô∏è  Skipped'}")
        print(f"   ‚è±Ô∏è  Total pipeline time:   {stats.get('pipeline_time', 0):.2f}s")

        # ‚îÄ‚îÄ Output Locations ‚îÄ‚îÄ
        print(f"\n{'‚îÄ'*70}")
        print("üìÅ OUTPUT LOCATIONS:")
        print("‚îÄ"*70)
        print(f"   ‚Ä¢ Chunks:       {results['chunks_folder']}")
        if results.get('embeddings_folder'):
            print(f"   ‚Ä¢ Embeddings:   {results['embeddings_folder']}")
        if results.get('vectordb_folder'):
            print(f"   ‚Ä¢ Vector DB:    {results['vectordb_folder']}")
        print(f"   ‚Ä¢ Logs:         {log_file.parent}")

        print(f"\n{'='*70}")
        print(f"‚úÖ Pipeline completed successfully!")
        print(f"üìù Full log: {log_file}")
        print(f"\nüí° Next step: Build the query/retrieval module!")
        print("="*70 + "\n")

        logger.info("Pipeline completed successfully")

    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Processing interrupted by user\n")
        logger.warning("Processing interrupted by user")

    except Exception as e:
        print(f"\n‚ùå ERROR: {str(e)}\n")
        logger.error(f"Pipeline failed: {str(e)}", exc_info=True)
        print(f"Check log file: {log_file}\n")

# ============================================================================
# ENTRY POINT
# ============================================================================

if __name__ == "__main__":
    main()